# Diverse Counterfactual Explanations in Machine Learning ğŸŒğŸ§ 

This project explores how to generate **diverse, actionable counterfactuals** to explain predictions from black-box models. It uses a custom implementation of **DiverseCF with Determinantal Point Processes (DPPs)** based on the DiCE library and the foundational paper by Microsoft Research. The counterfactuals are evaluated using key explainability metrics on a real-world Alzheimer's disease dataset.

---

## ğŸ“‚ Project Structure

- `notebook/dice1.ipynb` â€“ Jupyter notebook implementing DiverseCF + evaluation
- `outputs/` â€“ Screenshots showing CF examples, metrics, and console output

---

## ğŸ§ª Dataset Used

We used the **Alzheimer's Disease Dataset**, which contains clinical and diagnostic data for predicting Alzheimer's. This enables testing counterfactual explanations in a high-impact, real-world scenario.

- ğŸ“„ File: `dataset/alzheimers_disease_data.csv`
- ğŸ“Š Features: Age, education, MRI scores, MMSE, and more
- ğŸ¯ Target: Demented / Non-Demented
- ğŸ”— [Kaggle Link](https://www.kaggle.com/datasets/rabieelkharoua/alzheimers-disease-dataset)

> âš ï¸ Due to Kaggle's usage policy, the dataset is not uploaded directly. Users may download it from the link above.

---

## ğŸ§  About DiCE and the Research Paper

This project is inspired by the paper:  
**"Explaining Machine Learning Classifiers through Diverse Counterfactual Explanations"**  
*by Mothilal, Sharma (Microsoft Research India), and Tan (University of Colorado Boulder)*  
ğŸ”— [ACM Paper Link](https://doi.org/10.1145/3351095.3372850)

We used the **DiCE (Diverse Counterfactual Explanations)** library by Microsoft Research to generate counterfactuals. DiCE supports:
- Generating **multiple diverse counterfactuals**
- Applying **Determinantal Point Processes (DPPs)** to avoid repetitive outputs
- Respecting **causal constraints and feature immutability**
- Model-agnostic integration (works with scikit-learn, TensorFlow, etc.)

In our project:
- We **trained a classifier** on the Alzheimerâ€™s dataset
- Used DiCE to **generate and evaluate counterfactuals**
- Analyzed them using **validity, proximity, sparsity, and diversity**
- **Visualized results** with plots and text-based outputs

This implementation demonstrates the paperâ€™s central idea:  
âœ… Counterfactuals should be **valid**, **sparse**, **proximal**, and **diverse** â€” not just different, but **meaningfully and realistically different**.

---

## ğŸ“Š Evaluation Metrics

- **Validity** â€“ Do the CFs change the prediction?
- **Proximity** â€“ How close is a CF to the original input?
- **Sparsity** â€“ How few features are changed?
- **Diversity** â€“ How distinct are the generated CFs?

---

## ğŸ–¼ï¸ Output Visualizations

| Output | Description |
|--------|-------------|
| ![CF Comparison](outputs/diverse_cf_visualization.png) | ğŸ“Š Feature-wise bar chart showing 5 diverse CFs |
| ![Metrics](outputs/metrics_table.png) | ğŸ“ˆ Metric comparison: validity, proximity, sparsity, diversity |
| ![Text CFs](outputs/cf_text_explanations.png) | ğŸ§¾ Console output showing CFs at different proximity weights |


---

## ğŸ” Use Cases

- Explainable AI in healthcare diagnostics  
- Auditing model fairness and transparency  
- Developing user-friendly interpretable ML systems  
- Research in XAI, causality, and responsible AI

---

## ğŸ‘¨â€ğŸ’» Author

**Roshan A Rauof**  

---


